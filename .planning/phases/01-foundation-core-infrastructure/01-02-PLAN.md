---
phase: 01-foundation-core-infrastructure
plan: 02
type: tdd
wave: 2
depends_on:
  - 01
files_modified:
  - berryeval/metrics/_python.py
  - berryeval/metrics/_types.py
  - berryeval/metrics/_validation.py
  - berryeval/metrics/__init__.py
  - tests/metrics/test_recall.py
  - tests/metrics/test_precision.py
  - tests/metrics/test_mrr.py
  - tests/metrics/test_ndcg.py
  - tests/metrics/test_hit_rate.py
  - tests/metrics/test_validation.py
autonomous: true
requirements:
  - METR-08
  - NATV-04
  - NATV-05
  - NATV-06
  - PERF-03
must_haves:
  truths:
    - "recall_at_k produces correct scores for known test cases (perfect recall, zero recall, partial recall, k=1 boundary)"
    - "precision_at_k produces correct scores for known test cases"
    - "mrr produces correct scores including reciprocal rank at various positions"
    - "ndcg produces correct scores using binary relevance with proper DCG/IDCG computation"
    - "hit_rate produces correct binary scores (1.0 if any relevant doc in top-k, 0.0 otherwise)"
    - "All metrics handle edge cases: empty relevant sets, padding values, single query, large k"
    - "All metrics are deterministic: 100 runs with same input produce identical output"
    - "Input validation rejects wrong dtypes, non-contiguous arrays, shape mismatches"
    - "No input arrays are mutated by any metric function"
  artifacts:
    - path: "berryeval/metrics/_python.py"
      provides: "Pure Python implementations of all 5 IR metrics"
      exports: ["recall_at_k", "precision_at_k", "mrr", "ndcg", "hit_rate"]
      min_lines: 100
    - path: "berryeval/metrics/_validation.py"
      provides: "Input validation for NumPy array contracts"
      exports: ["validate_inputs", "validate_score_inputs"]
    - path: "berryeval/metrics/_types.py"
      provides: "Type definitions and constants"
      exports: ["PADDING_ID", "QueryDocArray", "RelevanceArray", "MetricResult"]
    - path: "tests/metrics/test_recall.py"
      provides: "Comprehensive recall@k test cases"
      min_lines: 50
    - path: "tests/metrics/test_precision.py"
      provides: "Comprehensive precision@k test cases"
      min_lines: 50
    - path: "tests/metrics/test_mrr.py"
      provides: "Comprehensive MRR test cases"
      min_lines: 50
    - path: "tests/metrics/test_ndcg.py"
      provides: "Comprehensive nDCG test cases"
      min_lines: 50
    - path: "tests/metrics/test_hit_rate.py"
      provides: "Comprehensive hit rate test cases"
      min_lines: 50
    - path: "tests/metrics/test_validation.py"
      provides: "Input validation error test cases"
      min_lines: 30
  key_links:
    - from: "berryeval/metrics/__init__.py"
      to: "berryeval/metrics/_python.py"
      via: "backend dispatch import"
      pattern: "from.*_python.*import"
    - from: "berryeval/metrics/__init__.py"
      to: "berryeval/metrics/_validation.py"
      via: "validation before computation"
      pattern: "validate_inputs"
    - from: "tests/metrics/test_recall.py"
      to: "berryeval/metrics"
      via: "public API import"
      pattern: "from berryeval.metrics import recall_at_k"
---

<objective>
Implement all five pure Python IR metrics (recall@k, precision@k, MRR, nDCG, hit rate) using TDD with comprehensive test coverage against known reference values.

Purpose: These implementations serve as both the always-available fallback (NATV-07) and the reference specification that future C kernels must match exactly. Correctness is paramount.
Output: Working, tested metric functions with full NumPy array interface contracts.
</objective>

<execution_context>
@/Users/benjaminmarks/.claude/get-shit-done/workflows/execute-plan.md
@/Users/benjaminmarks/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-foundation-core-infrastructure/01-RESEARCH.md
@.planning/phases/01-foundation-core-infrastructure/01-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement input validation and recall@k + precision@k with tests</name>
  <files>
    berryeval/metrics/_types.py
    berryeval/metrics/_validation.py
    berryeval/metrics/_python.py
    berryeval/metrics/__init__.py
    tests/metrics/test_validation.py
    tests/metrics/test_recall.py
    tests/metrics/test_precision.py
  </files>
  <action>
    **TDD approach: Write tests FIRST, then implement.**

    **tests/metrics/test_validation.py** — Test input validation:
    - test_rejects_non_numpy_input: Pass list instead of ndarray, expect TypeError
    - test_rejects_wrong_dtype: Pass float64 array, expect ValueError
    - test_rejects_1d_array: Pass 1D array, expect ValueError
    - test_rejects_non_contiguous: Pass Fortran-order array, expect ValueError
    - test_rejects_k_zero: k=0, expect ValueError
    - test_rejects_k_exceeds_columns: k > retrieved.shape[1], expect ValueError
    - test_rejects_query_count_mismatch: Different row counts, expect ValueError
    - test_accepts_valid_inputs: Valid arrays pass without error

    **tests/metrics/test_recall.py** — Test recall@k with parametrize:
    - perfect_recall: all relevant docs retrieved -> 1.0
    - zero_recall: no relevant docs retrieved -> 0.0
    - partial_recall: 2 of 3 relevant docs -> 2/3
    - recall_at_1: single position check
    - multi_query: multiple queries in one call
    - empty_relevant: no relevant docs (all padding) -> 0.0
    - with_padding: variable-length results with -1 padding
    - determinism: 100 identical calls produce identical results
    - no_mutation: input arrays unchanged after call

    **tests/metrics/test_precision.py** — Test precision@k with parametrize:
    - perfect_precision: all top-k are relevant -> 1.0
    - zero_precision: no top-k are relevant -> 0.0
    - partial_precision: 2 relevant in top-5 -> 0.4
    - precision_at_1: single position check
    - multi_query: multiple queries
    - with_padding: padding in retrieved (fewer results than k)
    - determinism and no_mutation tests

    Run tests — they should FAIL (RED phase).

    **berryeval/metrics/_types.py** — Complete the type definitions:
    - PADDING_ID: Final[int] = -1
    - Type aliases using numpy.typing.NDArray

    **berryeval/metrics/_validation.py** — Implement validate_inputs():
    - Check isinstance(arr, np.ndarray) for both arrays
    - Check dtype == np.int32
    - Check ndim == 2
    - Check flags['C_CONTIGUOUS']
    - Check k >= 1 and k <= retrieved.shape[1]
    - Check retrieved.shape[0] == relevant.shape[0]
    - Raise TypeError or ValueError with descriptive messages

    **berryeval/metrics/_python.py** — Implement recall_at_k and precision_at_k:

    recall_at_k(retrieved, relevant, k) -> MetricResult:
      For each query: |retrieved[:k] intersection relevant| / |relevant|
      Handle PADDING_ID (-1) by filtering before set operations
      Return 0.0 when relevant set is empty
      Use float64 intermediate, cast to float32 result

    precision_at_k(retrieved, relevant, k) -> MetricResult:
      For each query: |retrieved[:k] intersection relevant| / k
      Handle PADDING_ID by filtering
      When retrieved has fewer than k non-padding items, denominator = actual count (not k)
      Use float64 intermediate, cast to float32 result

    **berryeval/metrics/__init__.py** — Wire up the dispatch:
    - Import validate_inputs from _validation
    - Import functions from _python (try _native first with fallback)
    - Create public functions that validate then compute
    - Export BACKEND variable ("python" or "native")

    Run tests — they should PASS (GREEN phase).
  </action>
  <verify>
    ```bash
    pytest tests/metrics/test_validation.py tests/metrics/test_recall.py tests/metrics/test_precision.py -v
    ruff check berryeval/metrics/ tests/metrics/
    mypy berryeval/metrics/
    ```
  </verify>
  <done>
    - All validation tests pass (8+ test cases)
    - All recall@k tests pass (9+ test cases including edge cases)
    - All precision@k tests pass (7+ test cases including edge cases)
    - ruff and mypy pass on metrics module
    - No input arrays are mutated in any test
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement MRR, nDCG, and hit rate with tests</name>
  <files>
    berryeval/metrics/_python.py
    berryeval/metrics/__init__.py
    tests/metrics/test_mrr.py
    tests/metrics/test_ndcg.py
    tests/metrics/test_hit_rate.py
  </files>
  <action>
    **TDD approach: Write tests FIRST, then implement.**

    **tests/metrics/test_mrr.py** — Test MRR with parametrize:
    - first_position: relevant doc at rank 1 -> RR = 1.0
    - second_position: relevant doc at rank 2 -> RR = 0.5
    - fifth_position: relevant doc at rank 5 -> RR = 0.2
    - not_found: no relevant doc in top-k -> RR = 0.0
    - multi_query: average across queries
    - multiple_relevant: MRR uses FIRST relevant doc only
    - empty_relevant: no relevant docs -> 0.0
    - with_padding: padding in results
    - determinism and no_mutation tests

    **tests/metrics/test_ndcg.py** — Test nDCG with parametrize:
    - perfect_ranking: all relevant docs at top -> 1.0
    - worst_ranking: relevant doc at last position -> < 1.0 (compute exact expected value)
    - no_relevant_docs: empty relevant set -> 0.0
    - single_relevant_at_1: one relevant doc at position 1 -> 1.0
    - single_relevant_at_k: one relevant doc at position k -> computed value
    - multi_query: multiple queries with different results
    - binary_relevance_values: verify gain is 1.0 for relevant, 0.0 for not relevant
    - idcg_computation: verify ideal DCG is computed correctly
    - with_padding: padding in both arrays
    - determinism and no_mutation tests
    - Reference values: For k=5 with relevant={1,2,3} and retrieved=[1,4,2,5,3]:
      DCG = 1/log2(2) + 0/log2(3) + 1/log2(4) + 0/log2(5) + 1/log2(6) = 1.0 + 0.5 + 0.3869 = 1.8869
      IDCG = 1/log2(2) + 1/log2(3) + 1/log2(4) = 1.0 + 0.6309 + 0.5 = 2.1309
      nDCG = 1.8869 / 2.1309 = 0.8856

    **tests/metrics/test_hit_rate.py** — Test hit rate with parametrize:
    - hit: at least one relevant doc in top-k -> 1.0
    - miss: no relevant docs in top-k -> 0.0
    - hit_at_boundary: relevant doc at exactly position k -> 1.0
    - multi_query: mix of hits and misses
    - empty_relevant: no relevant docs -> 0.0
    - with_padding: padding handling
    - determinism and no_mutation tests

    Run tests — they should FAIL (RED phase).

    **berryeval/metrics/_python.py** — Add implementations:

    mrr(retrieved, relevant, k) -> MetricResult:
      For each query: find rank of FIRST relevant doc in retrieved[:k]
      Reciprocal rank = 1/rank (1-indexed). 0.0 if no relevant doc found.
      Use float64 intermediate, cast to float32 result.

    ndcg(retrieved, relevant, k) -> MetricResult:
      Binary relevance (gain = 1.0 for relevant, 0.0 for not).
      DCG@k = sum(gain_i / log2(i + 2)) for i in 0..k-1 (0-indexed, log2 uses i+2 for 1-indexed positions)
      IDCG@k = sum(1.0 / log2(i + 2)) for i in 0..min(n_relevant, k)-1
      nDCG = DCG / IDCG. Return 0.0 when IDCG = 0.
      Use float64 for ALL intermediate computation (log2, division). Cast final result to float32.
      Precompute discount factors: discounts = log2(arange(2, k+2)).

    hit_rate(retrieved, relevant, k) -> MetricResult:
      For each query: 1.0 if ANY relevant doc appears in retrieved[:k], else 0.0.
      Simpler than recall — binary per-query, no counting.

    **berryeval/metrics/__init__.py** — Add mrr, ndcg, hit_rate to dispatch and public exports.

    Run tests — they should PASS (GREEN phase).
  </action>
  <verify>
    ```bash
    pytest tests/metrics/ -v
    ruff check berryeval/metrics/ tests/metrics/
    mypy berryeval/metrics/
    python -c "from berryeval.metrics import recall_at_k, precision_at_k, mrr, ndcg, hit_rate, BACKEND; print(f'Backend: {BACKEND}')"
    ```
  </verify>
  <done>
    - All 5 metric test suites pass (40+ total test cases)
    - All edge cases covered: empty sets, padding, boundary k values
    - Determinism verified: 100 identical runs produce identical output
    - No mutation verified: input arrays unchanged after every metric call
    - ruff and mypy pass on entire metrics module
    - All metrics importable from berryeval.metrics public API
    - BACKEND reports "python" (no C extension available)
  </done>
</task>

</tasks>

<verification>
```bash
# Run full test suite with coverage
pytest tests/metrics/ -v --cov=berryeval.metrics --cov-report=term-missing

# Verify all metrics work end-to-end
python -c "
import numpy as np
from berryeval.metrics import recall_at_k, precision_at_k, mrr, ndcg, hit_rate

retrieved = np.array([[1, 2, 3, 4, 5]], dtype=np.int32)
relevant = np.array([[1, 3, 5, -1, -1]], dtype=np.int32)
k = 5

print(f'recall@{k}:    {recall_at_k(retrieved, relevant, k)}')
print(f'precision@{k}: {precision_at_k(retrieved, relevant, k)}')
print(f'mrr@{k}:       {mrr(retrieved, relevant, k)}')
print(f'ndcg@{k}:      {ndcg(retrieved, relevant, k)}')
print(f'hit_rate@{k}:  {hit_rate(retrieved, relevant, k)}')
"

# Verify determinism
python -c "
import numpy as np
from berryeval.metrics import recall_at_k
r = np.array([[1,2,3,4,5]], dtype=np.int32)
v = np.array([[1,3,5,-1,-1]], dtype=np.int32)
results = [recall_at_k(r, v, 5).tobytes() for _ in range(100)]
assert len(set(results)) == 1, 'NOT DETERMINISTIC!'
print('Determinism verified: 100 runs identical')
"
```
</verification>

<success_criteria>
- All 5 IR metrics implemented in pure Python using NumPy arrays
- 40+ test cases covering normal operation, edge cases, and error conditions
- All tests pass on Python 3.10+
- Code passes ruff check, ruff format --check, and mypy --strict
- Metrics are deterministic (verified by repeated execution)
- No input array mutation (verified by test assertions)
- NumPy array interface contract enforced: int32, 2D, C-contiguous
- PADDING_ID = -1 used consistently across all functions
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation-core-infrastructure/01-02-SUMMARY.md`
</output>
