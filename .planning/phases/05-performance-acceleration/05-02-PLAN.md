---
phase: 05-performance-acceleration
plan: 02
type: execute
wave: 2
depends_on:
  - 05-01
files_modified:
  - benchmarks/conftest.py
  - benchmarks/test_bench_metrics.py
  - pyproject.toml
autonomous: true
requirements:
  - NATV-08
  - PERF-01
  - PERF-02

must_haves:
  truths:
    - "Benchmarks measure wall-clock time for all 5 metrics at 1K, 10K, and 100K query scales"
    - "Benchmarks compare C backend vs Python backend performance on identical data"
    - "C backend achieves at least 5x speedup over Python backend on 10K+ queries"
    - "100K queries with top_k=50 completes in under 30 seconds with C backend"
    - "Benchmark results are reproducible via pytest-benchmark"
  artifacts:
    - path: "benchmarks/conftest.py"
      provides: "Shared fixtures for benchmark data generation at various scales"
      contains: "def make_benchmark_data"
    - path: "benchmarks/test_bench_metrics.py"
      provides: "Parametrized benchmark tests for all 5 metrics at 3 scales"
      contains: "benchmark"
  key_links:
    - from: "benchmarks/test_bench_metrics.py"
      to: "berryeval/metrics/_python.py"
      via: "direct import for Python-backend benchmarks"
      pattern: "from berryeval\\.metrics\\._python import"
    - from: "benchmarks/test_bench_metrics.py"
      to: "berryeval/metrics/_native"
      via: "direct import for C-backend benchmarks"
      pattern: "from berryeval\\.metrics\\._native import"
    - from: "benchmarks/test_bench_metrics.py"
      to: "berryeval/metrics"
      via: "public API benchmark for end-to-end timing"
      pattern: "from berryeval\\.metrics import"
---

<objective>
Create a benchmark suite that validates C acceleration performance targets: 5-10x improvement over pure Python and 100K queries with top_k=50 in under 30 seconds.

Purpose: Provide reproducible, quantitative evidence that the C kernels meet the performance requirements. Benchmarks compare Python vs C backends on identical synthetic data at realistic scales.
Output: benchmarks/ directory with pytest-benchmark tests producing comparison data for all 5 metrics.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/05-performance-acceleration/05-CONTEXT.md
@.planning/phases/05-performance-acceleration/05-01-SUMMARY.md
@berryeval/metrics/__init__.py
@berryeval/metrics/_python.py
@berryeval/metrics/_types.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Benchmark infrastructure and data fixtures</name>
  <files>
    pyproject.toml
    benchmarks/conftest.py
  </files>
  <action>
**pyproject.toml changes:**

Add pytest-benchmark to dev dependencies:
```toml
dev = [
    "ruff>=0.9.0",
    "mypy>=1.14.0",
    "pytest>=8.0",
    "pytest-cov>=6.0",
    "pytest-benchmark>=5.0",
]
```

Add benchmark configuration:
```toml
[tool.pytest.ini_options]
testpaths = ["tests"]
addopts = "-ra --strict-markers"
markers = [
    "benchmark: marks tests as benchmarks (deselect with '-m \"not benchmark\"')",
]
```

Note: Do NOT add benchmarks/ to testpaths. Benchmarks are run explicitly via `pytest benchmarks/` to avoid running them in normal CI test cycles.

**benchmarks/conftest.py:**

Create shared fixtures for generating benchmark data at various scales. The data must be realistic: random int32 arrays mimicking actual retriever output and ground truth.

```python
"""Shared fixtures for metric benchmarks."""
from __future__ import annotations

import numpy as np
import pytest

from berryeval.metrics._types import PADDING_ID


def make_benchmark_data(
    n_queries: int,
    n_retrieved: int = 50,
    n_relevant: int = 10,
    relevant_density: float = 0.5,
    seed: int = 42,
) -> tuple[np.ndarray, np.ndarray]:
    """Generate synthetic benchmark arrays.

    Args:
        n_queries: Number of query rows.
        n_retrieved: Columns in retrieved array (top_k capacity).
        n_relevant: Columns in relevant array.
        relevant_density: Fraction of relevant slots filled (rest = PADDING_ID).
        seed: Random seed for reproducibility.

    Returns:
        (retrieved, relevant) — both 2D int32 C-contiguous arrays.
    """
    rng = np.random.default_rng(seed)

    # Retrieved: random doc IDs from 1..100_000
    retrieved = rng.integers(1, 100_001, size=(n_queries, n_retrieved), dtype=np.int32)

    # Relevant: partially filled with doc IDs, rest padded with PADDING_ID
    relevant = np.full((n_queries, n_relevant), PADDING_ID, dtype=np.int32)
    n_filled = max(1, int(n_relevant * relevant_density))
    for i in range(n_queries):
        # Some relevant docs overlap with retrieved (ensures non-zero scores)
        n_overlap = max(1, n_filled // 3)
        overlap_ids = retrieved[i, :n_overlap].copy()
        random_ids = rng.integers(1, 100_001, size=n_filled - n_overlap, dtype=np.int32)
        relevant[i, :n_overlap] = overlap_ids
        relevant[i, n_overlap:n_filled] = random_ids

    return np.ascontiguousarray(retrieved), np.ascontiguousarray(relevant)


# Pre-built scale fixtures
SCALES = {
    "1K": 1_000,
    "10K": 10_000,
    "100K": 100_000,
}


@pytest.fixture(params=list(SCALES.keys()), scope="session")
def benchmark_data(request):
    """Parametrized fixture providing (retrieved, relevant, k, scale_name)."""
    scale_name = request.param
    n_queries = SCALES[scale_name]
    retrieved, relevant = make_benchmark_data(n_queries=n_queries, n_retrieved=50, n_relevant=10)
    return retrieved, relevant, 10, scale_name


@pytest.fixture(scope="session")
def data_100k():
    """Fixed 100K dataset for the absolute performance target test."""
    retrieved, relevant = make_benchmark_data(n_queries=100_000, n_retrieved=50, n_relevant=10)
    return retrieved, relevant
```

The `relevant_density=0.5` with deliberate overlap ensures benchmarks produce realistic (non-zero) metric scores, which matters for nDCG's IDCG computation path.
  </action>
  <verify>
cd /Users/benjaminmarks/Desktop/BerryEval && pip install -e ".[dev]" && python -c "from benchmarks.conftest import make_benchmark_data; r, v = make_benchmark_data(1000); print(f'Shape: retrieved={r.shape}, relevant={v.shape}, dtype={r.dtype}')"
  </verify>
  <done>
pytest-benchmark added to dev dependencies. benchmarks/conftest.py provides make_benchmark_data() fixture generating realistic synthetic arrays at 1K/10K/100K scales with controlled overlap ensuring non-zero metric scores.
  </done>
</task>

<task type="auto">
  <name>Task 2: Benchmark tests for all metrics with performance assertions</name>
  <files>
    benchmarks/test_bench_metrics.py
  </files>
  <action>
**benchmarks/test_bench_metrics.py:**

Create benchmark tests that:
1. Measure each metric's wall-clock time at 1K, 10K, 100K scales
2. Compare C backend vs Python backend directly
3. Assert the 5x minimum speedup target
4. Assert the 100K-in-30s absolute target

Structure: Import both backends directly to compare them side-by-side.

```python
"""Benchmark tests for metric computation performance.

Run with: pytest benchmarks/ -v --benchmark-enable
Comparison: pytest benchmarks/ -v --benchmark-enable --benchmark-group-by=param:scale_name
"""
from __future__ import annotations

import time

import numpy as np
import pytest

from benchmarks.conftest import SCALES, make_benchmark_data

# Import both backends directly for comparison
from berryeval.metrics import _python as py_backend

try:
    from berryeval.metrics import _native as c_backend  # type: ignore[attr-defined]
    HAS_NATIVE = True
except ImportError:
    HAS_NATIVE = False

METRICS = ["recall_at_k", "precision_at_k", "mrr", "ndcg", "hit_rate"]
```

**Parametrized backend comparison benchmarks:**

For each metric, create a benchmark that runs both Python and C backends:

```python
@pytest.mark.benchmark
@pytest.mark.parametrize("metric_name", METRICS)
@pytest.mark.parametrize("scale_name", list(SCALES.keys()))
def test_bench_python_backend(benchmark, metric_name, scale_name):
    """Benchmark pure Python backend."""
    n_queries = SCALES[scale_name]
    retrieved, relevant = make_benchmark_data(n_queries=n_queries)
    fn = getattr(py_backend, metric_name)
    benchmark(fn, retrieved, relevant, 10)


@pytest.mark.benchmark
@pytest.mark.parametrize("metric_name", METRICS)
@pytest.mark.parametrize("scale_name", list(SCALES.keys()))
def test_bench_native_backend(benchmark, metric_name, scale_name):
    """Benchmark C native backend."""
    if not HAS_NATIVE:
        pytest.skip("C extension not available")
    n_queries = SCALES[scale_name]
    retrieved, relevant = make_benchmark_data(n_queries=n_queries)
    fn = getattr(c_backend, metric_name)
    benchmark(fn, retrieved, relevant, 10)
```

**Speedup assertion test (not a benchmark — a functional performance test):**

```python
@pytest.mark.benchmark
@pytest.mark.parametrize("metric_name", METRICS)
def test_native_speedup_over_python(metric_name):
    """Assert C backend is at least 5x faster than Python on 10K queries."""
    if not HAS_NATIVE:
        pytest.skip("C extension not available")

    retrieved, relevant = make_benchmark_data(n_queries=10_000)
    py_fn = getattr(py_backend, metric_name)
    c_fn = getattr(c_backend, metric_name)

    # Warm up
    py_fn(retrieved, relevant, 10)
    c_fn(retrieved, relevant, 10)

    # Time Python backend (3 runs, take median)
    py_times = []
    for _ in range(3):
        start = time.perf_counter()
        py_fn(retrieved, relevant, 10)
        py_times.append(time.perf_counter() - start)
    py_median = sorted(py_times)[1]

    # Time C backend (3 runs, take median)
    c_times = []
    for _ in range(3):
        start = time.perf_counter()
        c_fn(retrieved, relevant, 10)
        c_times.append(time.perf_counter() - start)
    c_median = sorted(c_times)[1]

    speedup = py_median / c_median
    print(f"\n{metric_name}: Python={py_median:.4f}s, C={c_median:.4f}s, speedup={speedup:.1f}x")
    assert speedup >= 5.0, (
        f"{metric_name}: C backend only {speedup:.1f}x faster than Python "
        f"(target: >=5x). Python={py_median:.4f}s, C={c_median:.4f}s"
    )
```

**Absolute performance target test:**

```python
@pytest.mark.benchmark
def test_100k_queries_under_30_seconds():
    """Assert 100K queries with top_k=50 completes in <30 seconds for all metrics."""
    if not HAS_NATIVE:
        pytest.skip("C extension not available")

    retrieved, relevant = make_benchmark_data(n_queries=100_000, n_retrieved=50, n_relevant=10)
    k = 50

    # Import the public API (uses C backend when available)
    from berryeval.metrics import hit_rate, mrr, ndcg, precision_at_k, recall_at_k

    # Warm up
    recall_at_k(retrieved, relevant, k)

    # Time all 5 metrics sequentially (simulating full evaluation)
    start = time.perf_counter()
    recall_at_k(retrieved, relevant, k)
    precision_at_k(retrieved, relevant, k)
    mrr(retrieved, relevant, k)
    ndcg(retrieved, relevant, k)
    hit_rate(retrieved, relevant, k)
    total = time.perf_counter() - start

    print(f"\n100K queries x top_k=50, all 5 metrics: {total:.2f}s")
    assert total < 30.0, (
        f"All 5 metrics on 100K queries took {total:.2f}s (target: <30s)"
    )
```

**Correctness cross-check (ensure benchmarks use valid data):**

```python
@pytest.mark.benchmark
@pytest.mark.parametrize("metric_name", METRICS)
def test_backends_produce_same_results(metric_name):
    """Verify C and Python backends produce identical results on benchmark data."""
    if not HAS_NATIVE:
        pytest.skip("C extension not available")

    retrieved, relevant = make_benchmark_data(n_queries=1_000)
    py_fn = getattr(py_backend, metric_name)
    c_fn = getattr(c_backend, metric_name)

    py_result = py_fn(retrieved, relevant, 10)
    c_result = c_fn(retrieved, relevant, 10)

    np.testing.assert_allclose(
        c_result, py_result, atol=1e-5,
        err_msg=f"{metric_name}: C and Python backends disagree on benchmark data"
    )
```
  </action>
  <verify>
cd /Users/benjaminmarks/Desktop/BerryEval && python -m pytest benchmarks/test_bench_metrics.py -v -k "test_backends_produce_same_results" && python -m pytest benchmarks/test_bench_metrics.py -v -k "test_native_speedup" && python -m pytest benchmarks/test_bench_metrics.py -v -k "test_100k"
  </verify>
  <done>
Benchmark suite validates: (1) C and Python backends produce identical results on benchmark data, (2) C backend achieves at least 5x speedup over Python on 10K queries for all 5 metrics, (3) all 5 metrics on 100K queries with top_k=50 complete in under 30 seconds. pytest-benchmark integration enables detailed timing comparisons via `pytest benchmarks/ --benchmark-enable`.
  </done>
</task>

</tasks>

<verification>
```bash
# Correctness cross-check
cd /Users/benjaminmarks/Desktop/BerryEval && python -m pytest benchmarks/test_bench_metrics.py -v -k "test_backends_produce_same_results"

# Speedup validation (5x+ for each metric)
python -m pytest benchmarks/test_bench_metrics.py -v -k "test_native_speedup"

# Absolute performance target (100K queries < 30s)
python -m pytest benchmarks/test_bench_metrics.py -v -k "test_100k"

# Full benchmark suite with timing details
python -m pytest benchmarks/ -v --benchmark-enable

# Existing tests still pass
python -m pytest tests/ -v
```
</verification>

<success_criteria>
- Benchmarks run for all 5 metrics at 1K, 10K, 100K scales
- C backend produces identical results to Python backend on benchmark data
- C backend achieves at least 5x speedup over Python for all 5 metrics at 10K scale
- 100K queries with top_k=50, all 5 metrics complete in under 30 seconds
- Benchmark results are reproducible (fixed seeds, deterministic data)
</success_criteria>

<output>
After completion, create `.planning/phases/05-performance-acceleration/05-02-SUMMARY.md`
</output>
