---
phase: 05-performance-acceleration
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - setup.py
  - pyproject.toml
  - native/src/metrics.h
  - native/src/metrics.c
  - native/bindings/module.c
  - berryeval/metrics/_native.py
autonomous: true
requirements:
  - NATV-01
  - NATV-02
  - NATV-03

must_haves:
  truths:
    - "C extension compiles and installs as berryeval.metrics._native module"
    - "All 5 metric functions (recall_at_k, precision_at_k, mrr, ndcg, hit_rate) are callable from Python via the C module"
    - "C kernels produce numerically identical results to pure Python implementations (within float32 tolerance)"
    - "BACKEND reports 'native' when C extension is installed"
    - "Existing metric test suite passes with C backend active"
    - "C kernels never mutate input arrays"
    - "Ranking position lookup helper is used internally by mrr kernel"
  artifacts:
    - path: "setup.py"
      provides: "setuptools Extension() definition for C extension compilation"
      contains: "Extension"
    - path: "native/src/metrics.h"
      provides: "C function declarations for all 5 metric kernels plus rank_lookup helper"
      contains: "berry_recall_at_k"
    - path: "native/src/metrics.c"
      provides: "Pure C kernel implementations matching _python.py algorithms exactly"
      contains: "berry_ndcg"
    - path: "native/bindings/module.c"
      provides: "Python C extension module wrapping kernels via NumPy C API"
      contains: "import_array"
  key_links:
    - from: "native/bindings/module.c"
      to: "native/src/metrics.h"
      via: "includes header and calls C kernel functions"
      pattern: "#include.*metrics\\.h"
    - from: "native/bindings/module.c"
      to: "berryeval/metrics/__init__.py"
      via: "module name berryeval.metrics._native matches import"
      pattern: "berryeval\\.metrics\\._native"
    - from: "setup.py"
      to: "native/src/metrics.c"
      via: "Extension() sources list references C files"
      pattern: "native/src/metrics\\.c"
    - from: "berryeval/metrics/__init__.py"
      to: "berryeval/metrics/_native"
      via: "try/except import dispatches to C or Python backend"
      pattern: "from berryeval\\.metrics\\._native import"
---

<objective>
Build the C acceleration layer: setup.py build system, pure C metric kernels for all 5 IR metrics, and the Python C extension module that bridges them into the existing dispatch mechanism.

Purpose: Replace the _native.py ImportError stub with actual compiled C kernels so that `berryeval.metrics.__init__.py` successfully imports the native backend. The existing test suite validates correctness automatically since tests go through the public API.
Output: Compilable C extension producing berryeval.metrics._native, verified by existing metric tests reporting BACKEND="native".
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/05-performance-acceleration/05-CONTEXT.md
@berryeval/metrics/__init__.py
@berryeval/metrics/_python.py
@berryeval/metrics/_types.py
@berryeval/metrics/_validation.py
@pyproject.toml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Build system and C kernel implementation</name>
  <files>
    setup.py
    pyproject.toml
    native/src/metrics.h
    native/src/metrics.c
  </files>
  <action>
**pyproject.toml changes:**

Add numpy to build-requires so NumPy headers are available during compilation:
```toml
[build-system]
requires = ["setuptools>=75.0", "numpy>=2.0"]
build-backend = "setuptools.build_meta"
```

No other changes to pyproject.toml.

**setup.py:**

Create a setup.py at project root that defines the C extension. This file is used by setuptools alongside pyproject.toml. It ONLY defines ext_modules — all other metadata stays in pyproject.toml.

```python
"""Build configuration for C extension module."""
import sys

import numpy as np
from setuptools import Extension, setup

# Platform-specific compiler flags
if sys.platform == "win32":
    extra_compile_args = ["/O2"]
else:
    extra_compile_args = ["-O2", "-std=c99"]

ext_modules = [
    Extension(
        name="berryeval.metrics._native",
        sources=[
            "native/src/metrics.c",
            "native/bindings/module.c",
        ],
        include_dirs=[
            "native/src",
            np.get_include(),
        ],
        extra_compile_args=extra_compile_args,
    ),
]

setup(ext_modules=ext_modules)
```

Notes:
- Use -O2 (safe optimization, not -O3 which can cause subtle issues)
- Use -std=c99 for portability on Unix; /O2 for MSVC on Windows
- The `name` must be exactly `berryeval.metrics._native` to match the import in __init__.py

**native/src/metrics.h:**

Header file declaring all C kernel functions. Use `berry_` prefix to avoid name collisions.

```c
#ifndef BERRYEVAL_METRICS_H
#define BERRYEVAL_METRICS_H

#include <stdint.h>

/*
 * All kernel functions operate on raw pointers.
 * Inputs are NEVER mutated.
 *
 * Parameters:
 *   retrieved  - 2D int32 array, shape (n_queries, n_retrieved), row-major
 *   relevant   - 2D int32 array, shape (n_queries, n_relevant), row-major
 *   out        - 1D float array, shape (n_queries,), pre-allocated by caller
 *   n_queries  - number of query rows
 *   n_retrieved - number of columns in retrieved array
 *   n_relevant  - number of columns in relevant array
 *   k          - cutoff for top-k evaluation
 *
 * PADDING_ID = -1 in relevant arrays means "no document" (skip).
 */

#define BERRY_PADDING_ID (-1)

/* Ranking position lookup: find first position of any relevant doc in retrieved row.
 * Returns 1-based rank (1 = first position), or 0 if not found.
 * Searches retrieved[0..k-1] for any value in relevant[0..n_relevant-1] that != PADDING_ID.
 */
int berry_rank_lookup(
    const int32_t *retrieved_row,
    int k,
    const int32_t *relevant_row,
    int n_relevant
);

void berry_recall_at_k(
    const int32_t *retrieved,
    const int32_t *relevant,
    float *out,
    int n_queries,
    int n_retrieved,
    int n_relevant,
    int k
);

void berry_precision_at_k(
    const int32_t *retrieved,
    const int32_t *relevant,
    float *out,
    int n_queries,
    int n_retrieved,
    int n_relevant,
    int k
);

void berry_mrr(
    const int32_t *retrieved,
    const int32_t *relevant,
    float *out,
    int n_queries,
    int n_retrieved,
    int n_relevant,
    int k
);

void berry_ndcg(
    const int32_t *retrieved,
    const int32_t *relevant,
    float *out,
    int n_queries,
    int n_retrieved,
    int n_relevant,
    int k
);

void berry_hit_rate(
    const int32_t *retrieved,
    const int32_t *relevant,
    float *out,
    int n_queries,
    int n_retrieved,
    int n_relevant,
    int k
);

#endif /* BERRYEVAL_METRICS_H */
```

**native/src/metrics.c:**

Pure C implementations. Each kernel iterates over n_queries rows. MUST match the exact algorithms in berryeval/metrics/_python.py. The key patterns from _python.py:

1. **recall_at_k**: For each query i, build set of relevant IDs (excluding PADDING_ID), build set of retrieved[:k] IDs, score = |intersection| / |relevant_set| (0.0 if relevant_set empty).

2. **precision_at_k**: Same sets, score = |intersection| / k.

3. **mrr**: For each query i, build relevant set (excluding PADDING_ID). If empty, score=0. Else iterate retrieved[i,0..k-1]; first match at rank r (1-based) gives score=1.0/r. Uses berry_rank_lookup.

4. **ndcg**: For each query i, build relevant set (excluding PADDING_ID). Count n_rel = |set|. If 0, score=0. Compute DCG: for j in 0..k-1, if retrieved[i,j] in relevant_set, dcg += 1.0/log2(j+2). Compute IDCG: for j in 0..min(n_rel,k)-1, idcg += 1.0/log2(j+2). Score = dcg/idcg.

5. **hit_rate**: For each query i, build relevant set (excluding PADDING_ID). If empty, score=0. Else check if any retrieved[i,0..k-1] is in relevant set. Score = 1.0 or 0.0.

For the "build set and check membership" pattern in C, use a simple linear scan since relevant arrays are small (typically <50 elements). No hash table needed:

```c
/* Helper: check if val exists in arr[0..len-1], ignoring PADDING_ID entries */
static int is_in_relevant(int32_t val, const int32_t *arr, int len) {
    for (int j = 0; j < len; j++) {
        if (arr[j] == BERRY_PADDING_ID) continue;
        if (arr[j] == val) return 1;
    }
    return 0;
}

/* Helper: count non-padding entries */
static int count_relevant(const int32_t *arr, int len) {
    int count = 0;
    for (int j = 0; j < len; j++) {
        if (arr[j] != BERRY_PADDING_ID) count++;
    }
    return count;
}

/* Helper: count how many unique relevant items appear in retrieved[:k].
 * CRITICAL: This matches Python's set() semantics — if retrieved has duplicates,
 * each relevant item is counted at most once (not once per occurrence).
 * Approach: iterate over relevant items, check if each appears in retrieved[:k].
 * This naturally deduplicates because each relevant item is checked exactly once.
 */
static int count_set_intersection(
    const int32_t *retrieved_row, int k,
    const int32_t *relevant_row, int n_relevant
) {
    int hits = 0;
    for (int r = 0; r < n_relevant; r++) {
        int32_t rel_id = relevant_row[r];
        if (rel_id == BERRY_PADDING_ID) continue;
        /* Check if this relevant doc appears anywhere in retrieved[:k] */
        for (int j = 0; j < k; j++) {
            if (retrieved_row[j] == rel_id) {
                hits++;
                break;  /* found — count once, move to next relevant item */
            }
        }
    }
    return hits;
}
```

Include `<math.h>` for log2(). Include `<stdint.h>` and `"metrics.h"`.

**berry_rank_lookup implementation:**
```c
int berry_rank_lookup(
    const int32_t *retrieved_row,
    int k,
    const int32_t *relevant_row,
    int n_relevant
) {
    for (int j = 0; j < k; j++) {
        if (is_in_relevant(retrieved_row[j], relevant_row, n_relevant)) {
            return j + 1;  /* 1-based rank */
        }
    }
    return 0;  /* not found */
}
```

**berry_recall_at_k implementation:**
For each query i:
- `const int32_t *ret_row = retrieved + i * n_retrieved;`
- `const int32_t *rel_row = relevant + i * n_relevant;`
- `int n_rel = count_relevant(rel_row, n_relevant);`
- If n_rel == 0: out[i] = 0.0f; continue
- `int hits = count_set_intersection(ret_row, k, rel_row, n_relevant);`
- out[i] = (float)hits / (float)n_rel;

**IMPORTANT:** Uses `count_set_intersection()` which iterates over relevant items and checks presence in retrieved[:k]. This matches Python's `set(retrieved[:k]) & set(relevant)` semantics exactly — each relevant item is counted at most once even if retrieved has duplicate IDs.

**berry_precision_at_k implementation:**
Same intersection counting as recall: `int hits = count_set_intersection(ret_row, k, rel_row, n_relevant);`
Score: out[i] = (float)hits / (float)k;

**berry_mrr implementation:**
For each query i:
- n_rel = count_relevant(rel_row, n_relevant). If 0: out[i] = 0.0f; continue.
- rank = berry_rank_lookup(ret_row, k, rel_row, n_relevant)
- out[i] = (rank > 0) ? 1.0f / (float)rank : 0.0f;

**berry_ndcg implementation:**
For each query i:
- n_rel = count_relevant(rel_row, n_relevant). If 0: out[i] = 0.0f; continue.
- dcg = 0.0 (use double for intermediate precision, matching Python's float64 intermediates)
- for j in 0..k-1: if is_in_relevant(ret_row[j], rel_row, n_relevant) then dcg += 1.0 / log2((double)(j + 2))
- ideal_count = min(n_rel, k)
- idcg = 0.0
- for j in 0..ideal_count-1: idcg += 1.0 / log2((double)(j + 2))
- out[i] = (idcg > 0.0) ? (float)(dcg / idcg) : 0.0f;

**berry_hit_rate implementation:**
For each query i:
- n_rel = count_relevant(rel_row, n_relevant). If 0: out[i] = 0.0f; continue.
- hit = 0
- for j in 0..k-1: if is_in_relevant(ret_row[j], rel_row, n_relevant) { hit = 1; break; }
- out[i] = (float)hit;

**CRITICAL:** Use double for intermediate accumulations (dcg, idcg) to match Python's float64 intermediates. Only cast to float at final assignment to out[i]. This ensures numerical equivalence with the Python reference.
  </action>
  <verify>
cd /Users/benjaminmarks/Desktop/BerryEval && pip install -e ".[dev]" 2>&1 | tail -5 && python -c "from berryeval.metrics._native import recall_at_k; print('C extension compiled and importable')"
  </verify>
  <done>
setup.py defines Extension() for berryeval.metrics._native. pyproject.toml includes numpy in build-requires. native/src/metrics.h declares all 6 functions (5 metrics + rank_lookup). native/src/metrics.c implements all kernels matching _python.py algorithms. `pip install -e .` compiles the extension successfully.
  </done>
</task>

<task type="auto">
  <name>Task 2: Python C extension bindings and integration</name>
  <files>
    native/bindings/module.c
    berryeval/metrics/_native.py
  </files>
  <action>
**native/bindings/module.c:**

Python C extension module that wraps the C kernels as Python-callable functions using NumPy C API.

```c
#define PY_SSIZE_T_CLEAN
#include <Python.h>

#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION
#include <numpy/arrayobject.h>

#include "metrics.h"
```

For each of the 5 metric functions, create a wrapper function following this pattern (shown for recall_at_k):

```c
static PyObject *
py_recall_at_k(PyObject *self, PyObject *args)
{
    PyArrayObject *retrieved_obj, *relevant_obj;
    int k;

    /* Parse args: two NumPy arrays + int k */
    if (!PyArg_ParseTuple(args, "O!O!i",
                          &PyArray_Type, &retrieved_obj,
                          &PyArray_Type, &relevant_obj,
                          &k))
        return NULL;

    /* Extract dimensions */
    int n_queries = (int)PyArray_DIM(retrieved_obj, 0);
    int n_retrieved = (int)PyArray_DIM(retrieved_obj, 1);
    int n_relevant = (int)PyArray_DIM(relevant_obj, 1);

    /* Get data pointers (validation already done by Python layer) */
    const int32_t *retrieved = (const int32_t *)PyArray_DATA(retrieved_obj);
    const int32_t *relevant = (const int32_t *)PyArray_DATA(relevant_obj);

    /* Create output array: 1D float32, shape (n_queries,) */
    npy_intp dims[1] = {n_queries};
    PyObject *out_obj = PyArray_SimpleNew(1, dims, NPY_FLOAT32);
    if (out_obj == NULL)
        return NULL;

    float *out = (float *)PyArray_DATA((PyArrayObject *)out_obj);

    /* Call C kernel */
    berry_recall_at_k(retrieved, relevant, out, n_queries, n_retrieved, n_relevant, k);

    return out_obj;
}
```

Create identical wrappers for: py_precision_at_k, py_mrr, py_ndcg, py_hit_rate. Each calls the corresponding berry_* function. The function signatures and argument parsing are identical across all 5.

**Method table:**
```c
static PyMethodDef MetricsMethods[] = {
    {"recall_at_k",    py_recall_at_k,    METH_VARARGS, "Compute recall@k for each query."},
    {"precision_at_k", py_precision_at_k, METH_VARARGS, "Compute precision@k for each query."},
    {"mrr",            py_mrr,            METH_VARARGS, "Compute MRR for each query."},
    {"ndcg",           py_ndcg,           METH_VARARGS, "Compute nDCG for each query."},
    {"hit_rate",       py_hit_rate,       METH_VARARGS, "Compute hit rate for each query."},
    {NULL, NULL, 0, NULL}
};
```

**Module definition:**
```c
static struct PyModuleDef metricsmodule = {
    PyModuleDef_HEAD_INIT,
    "berryeval.metrics._native",   /* module name — matches import path */
    "C-accelerated IR metric kernels for BerryEval.",
    -1,
    MetricsMethods
};

PyMODINIT_FUNC
PyInit__native(void)
{
    import_array();  /* Initialize NumPy C API — MUST be called */
    return PyModule_Create(&metricsmodule);
}
```

IMPORTANT: The init function MUST be named `PyInit__native` (double underscore — matches the `_native` part of the module name `berryeval.metrics._native`).

**Remove berryeval/metrics/_native.py:**

Delete this file entirely. When the C extension is compiled, the .so (or .pyd) file will be found at `berryeval/metrics/_native.cpython-3XX-*.so`. When the C extension is NOT compiled (e.g., pure sdist install without compiler), the import in __init__.py will raise ImportError naturally since neither .py nor .so exists, triggering the Python fallback. This is the intended behavior per the CONTEXT.md decisions.

After deleting _native.py, reinstall with `pip install -e ".[dev]"` to compile the C extension.
  </action>
  <verify>
cd /Users/benjaminmarks/Desktop/BerryEval && pip install -e ".[dev]" 2>&1 | tail -5 && python -c "from berryeval.metrics import get_backend; assert get_backend() == 'native', f'Expected native, got {get_backend()}'; print(f'BACKEND = {get_backend()}')" && python -m pytest tests/metrics/ -v
  </verify>
  <done>
C extension module compiles to berryeval.metrics._native. _native.py stub is removed. get_backend() returns "native". All existing metric tests (test_recall.py, test_precision.py, test_mrr.py, test_ndcg.py, test_hit_rate.py, test_validation.py) pass with C backend active — confirming numerical equivalence with Python reference. test_no_mutation tests confirm C never mutates inputs. test_result_dtype tests confirm float32 output.
  </done>
</task>

</tasks>

<verification>
```bash
# C extension compiles
cd /Users/benjaminmarks/Desktop/BerryEval && pip install -e ".[dev]"

# Backend is native
python -c "from berryeval.metrics import get_backend; print(get_backend())"
# Expected: "native"

# All metric tests pass with C backend
python -m pytest tests/metrics/ -v

# Full test suite passes
python -m pytest tests/ -v

# Type checking still clean
python -m mypy berryeval

# Linting still clean
python -m ruff check .
```
</verification>

<success_criteria>
- setup.py defines Extension() for berryeval.metrics._native with correct sources and include_dirs
- pyproject.toml build-requires includes numpy for header access
- C kernels implement all 5 metrics + rank_lookup helper matching _python.py exactly
- Python C extension module wraps all 5 kernels via NumPy C API
- _native.py stub removed; C .so replaces it
- get_backend() returns "native" after installation
- All existing metric tests pass unchanged with C backend active
- No input mutation, float32 output, deterministic results
</success_criteria>

<output>
After completion, create `.planning/phases/05-performance-acceleration/05-01-SUMMARY.md`
</output>
